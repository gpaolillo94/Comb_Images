{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install opencv-python\n",
    "!pip install staintools\n",
    "!pip install colorcorrect\n",
    "!pip install spams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UwIaFXXiuCH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading libraries\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import staintools\n",
    "import colorcorrect.algorithm as cca\n",
    "from colorcorrect.util import from_pil, to_pil\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVrJvRF7TGEK"
   },
   "outputs": [],
   "source": [
    "#define function saving and loading pickle\n",
    "def save_obj(obj, dir_save, file_name ):\n",
    "    if not os.path.exists(dir_save):\n",
    "        os.mkdir(dir_save)\n",
    "    \n",
    "    with open(os.path.join(dir_save, file_name + '.pckl'), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(dir_save, file_name):\n",
    "    if os.path.exists(os.path.join(dir_save, file_name + '.pckl')):\n",
    "        \n",
    "        with open(os.path.join(dir_save, file_name + '.pckl'), 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pjp4RfhiuCM",
    "outputId": "5280ecf0-c8b1-4a11-c888-af579ce0fab9"
   },
   "outputs": [],
   "source": [
    "# setting paths\n",
    "path = os.getcwd()\n",
    "\n",
    "experiment = 'Circular_Crop' #name of folder of the experiment. Used 'Circular_Crop', 'DeepBee', 'Internal_Crop'\n",
    "data_folder = 'Data'\n",
    "result_folder = 'Results'\n",
    "\n",
    "root_path = os.path.join(path,experiment)\n",
    "\n",
    "data_path = os.path.join(root_path,data_folder)\n",
    "photo_path = os.path.join(root_path,data_folder,'photos')\n",
    "\n",
    "result_path = os.path.join(root_path,result_folder)\n",
    "\n",
    "if not os.path.exists(os.path.join(result_path,'resized')):\n",
    "    os.mkdir(os.path.join(result_path,'resized'))\n",
    "\n",
    "\n",
    "result_path_objs = os.path.join(result_path, 'objs')\n",
    "if not os.path.exists(result_path_objs):\n",
    "    os.mkdir(result_path_objs)\n",
    "unique_Labels_List = os.listdir(data_path)\n",
    "\n",
    "\n",
    "\n",
    "data_dir = data_path\n",
    "os.listdir(root_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "174Acmn9iuCO"
   },
   "outputs": [],
   "source": [
    "# Set Params of Pipeline\n",
    "normalization = 'ACE' #methods available 'ACE', 'MACENKO', 'VAHADANE' \n",
    "resize = 0.1 #for experiment Crop = 0.1, for DeepBee = 0.5\n",
    "thresholding = 'OTSU' #methods available 'OTSU', 'ADAPTIVE_MEAN', 'ADAPTIVE_GAUSSIAN' \n",
    "min_Radius = 1 #set values for circle detection\n",
    "max_Radius = 27 #set values for circle detection\n",
    "blocksize = 7 #only adaptive thresholding\n",
    "constant = 26 #only adaptive thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZcVes_8iuCh"
   },
   "outputs": [],
   "source": [
    "###########################Circle Detection function + correlation#######################\n",
    "def thresholding_Hough_correl(norm_dict, thresholding, min_Radius=1, max_Radius = 27, blocksize = 11 , constant = 0):\n",
    "    \n",
    "    res_img_dict = {}\n",
    "    circle_dict = {}\n",
    "    masked_img_dict = {}\n",
    "    img_area_dict = {}\n",
    "    circle_folder = 'Circle_Detection'\n",
    "    circle_result_path = os.path.join(result_path, circle_folder)\n",
    "    maxRadius_correl_dict = {}\n",
    "\n",
    "    if (min_Radius >= max_Radius):\n",
    "        correl = 0\n",
    "    else:\n",
    "        n_tot_cells = 110\n",
    "\n",
    "        print(\"working on thresholding method = \"+ thresholding + \", blocksize = \"+  str(blocksize) + \" - C = \"+ str(constant) + \"\\n\"+\n",
    "            \" - Hough minR = \"+ str(min_Radius) + \", maxR = \"+ str(max_Radius))\n",
    "\n",
    "        for key in norm_dict.keys():\n",
    "            #####################################Circle Detection######################################\n",
    "            #RGB2Gray conversion\n",
    "            gray = cv2.cvtColor(norm_dict[key], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            #Thresholding\n",
    "            if thresholding == 'ADAPTIVE_MEAN':\n",
    "        #        th = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,11,np.mean(norm_dict[key])*0.01)\n",
    "                th = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,blocksize,constant)\n",
    "            elif thresholding == 'ADAPTIVE_GAUSSIAN':\n",
    "                th = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,blocksize,constant)\n",
    "            elif thresholding == 'OTSU':\n",
    "                ret,th = cv2.threshold(gray,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "            else:\n",
    "                print('continue')\n",
    "                continue\n",
    "\n",
    "            res_img_dict[key] = th\n",
    "\n",
    "            #Save inverted Otsu's mask\n",
    "            inverted_img = 255-th\n",
    "            path_to_Otsu_img = os.path.join(circle_result_path, 'Mask', key)\n",
    "            plt.imsave(path_to_Otsu_img, inverted_img, cmap = 'gray')\n",
    "\n",
    "            #Hough Circle Detection on Mask\n",
    "            \n",
    "            circles = cv2.HoughCircles(inverted_img,cv2.HOUGH_GRADIENT,1, 10,\n",
    "                                    param1=25,param2=15,minRadius=min_Radius,maxRadius=max_Radius)\n",
    "            \n",
    "            if circles is not None:\n",
    "                circles = np.uint16(np.around(circles))\n",
    "                \n",
    "                circle_dict[key] = circles\n",
    "\n",
    "                #Draw Circle on mask\n",
    "                input_mask = path_to_Otsu_img\n",
    "                img_test = cv2.imread(input_mask)\n",
    "\n",
    "                # ensure at least some circles were found\n",
    "                # convert the (x, y) coordinates and radius of the circles to integers\n",
    "                circles = np.round(circles[0, :]).astype(\"int\")\n",
    "                \n",
    "                \n",
    "                # loop over the (x, y) coordinates and radius of the circles\n",
    "                for (x, y, r) in circles:\n",
    "                    # draw the circle in the output image, then draw a rectangle\n",
    "                    # corresponding to the center of the circle\n",
    "                    cv2.circle(img_test, (x, y), r, (255, 0, 0), 2) \n",
    "        \n",
    "                circled_img = img_test\n",
    "\n",
    "                #Save Circled Image\n",
    "                path_to_result_img = os.path.join(circle_result_path, 'Circled_Img', key)\n",
    "                plt.imsave(path_to_result_img, circled_img, cmap = 'gray')\n",
    "\n",
    "                #Measure Cells count\n",
    "                n_empty_cells = len(circles)\n",
    "                if experiment == 'Circular_Crop':\n",
    "                    n_t24 = n_tot_cells - n_empty_cells\n",
    "                else:\n",
    "                    n_t24 = n_empty_cells\n",
    " #               print(n_t24)\n",
    "            else:\n",
    " #               print('no circles found')\n",
    "                n_t24 = None\n",
    "\n",
    "            img_area_dict[key] = n_t24\n",
    "\n",
    "\n",
    "        img_area_dict.keys()\n",
    "\n",
    "        # EXPORT COUNTS to dataframe where img name is the index\n",
    "        img_area_df = pd.DataFrame.from_dict(img_area_dict, orient = 'index')\n",
    "\n",
    "        # IMPORT MANUAL COUNTS\n",
    "        df = pd.read_csv(os.path.join(data_path,'Correl.csv'), sep = '\\t')\n",
    "\n",
    "        #dataframe where img name is the index\n",
    "        df.index = df['foto']\n",
    "        \n",
    "        print(\"merging the dataframes\")\n",
    "        # merge the two dataframe\n",
    "        df_merge = pd.merge(df, img_area_df, left_index= True, right_index= True, how = 'inner')\n",
    "        print(df_merge.shape)\n",
    "        \n",
    "        df_merge.columns = ('foto', 'manual', 'auto')\n",
    "        df_merge = df_merge.dropna()\n",
    "        if df_merge.shape[0]>3:\n",
    "            r = np.corrcoef(df_merge[\"manual\"].astype('int'), df_merge[\"auto\"].astype('int'))\n",
    "            correl = r[0,1]\n",
    "            if not np.isfinite(correl):\n",
    "                print(\"correl is nan...\")\n",
    "                correl = 0\n",
    "        else: \n",
    "            correl = 0\n",
    "    return correl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggKIPXeXiuCR",
    "outputId": "7cc8eba3-da8d-4417-ba7f-4287ead53cb1"
   },
   "outputs": [],
   "source": [
    "######################## Import Images ###############################à\n",
    "img_dict = load_obj(dir_save=result_path_objs, file_name='img_dict')\n",
    "\n",
    "if img_dict ==None:\n",
    "    img_dict = load_obj(result_path_objs, 'img_dict')\n",
    "\n",
    "    if img_dict ==None:\n",
    "        img_dict = {}\n",
    "        for filename in os.listdir(photo_path):\n",
    "            path_to_image = os.path.join(photo_path, filename)\n",
    "            image = cv2.imread(path_to_image)\n",
    "            img_dict[filename] = image\n",
    "    \n",
    "    save_obj(dir_save=result_path_objs, file_name='img_dict', obj = img_dict)\n",
    "    \n",
    "\n",
    "print(img_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXc1vYUJiuCS"
   },
   "outputs": [],
   "source": [
    "#########################Normalization######################\n",
    "normalization == 'ACE'\n",
    "\n",
    "if not os.path.exists(os.path.join(result_path,'gaussian_filter')):\n",
    "    os.mkdir(os.path.join(result_path,'gaussian_filter'))\n",
    "\n",
    "dir_smooth = os.path.join(result_path,'gaussian_filter')\n",
    "\n",
    "if not os.path.exists(os.path.join(result_path,'Circle_Detection')):\n",
    "    os.mkdir(os.path.join(result_path,'Circle_Detection'))\n",
    "    os.mkdir(os.path.join(result_path,'Circle_Detection','Mask'))\n",
    "    os.mkdir(os.path.join(result_path,'Circle_Detection','Circled_Img'))\n",
    "    os.mkdir(os.path.join(result_path,'Circle_Detection','Masked_Circle_Img'))\n",
    "\n",
    "if not os.path.exists(os.path.join(result_path,normalization + '_normalized')):\n",
    "    os.mkdir(os.path.join(result_path,normalization + '_normalized'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PvxRdN7giuCY",
    "outputId": "9e4b924d-5752-4d8f-c49b-27dc507e301b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########################Normalization######################\n",
    "norm_dict = {}\n",
    "target_path = os.path.join(data_path,'target_norm/DSC00037.JPG')\n",
    "\n",
    "for key in img_dict.keys():\n",
    "    print(key)\n",
    "\n",
    "    path_to_result_img = os.path.join(dir_smooth, key)\n",
    "\n",
    "    #Filtering\n",
    "    if os.path.exists(path_to_result_img):\n",
    "        print(key + \" after gaussian exists: I'm reading it\")\n",
    "        gauss_img = cv2.imread(path_to_result_img)\n",
    "    else:\n",
    "        print(key + \" after gaussian does not exist: I'm creating it\")\n",
    "        mf_img = cv2.medianBlur(img_dict[key], 5)     #median filter\n",
    "        gauss_img = cv2.GaussianBlur(mf_img, (3,3), 5) # 3x3 gaussian blur\n",
    "        plt.imsave(path_to_result_img, gauss_img)\n",
    "\n",
    "    if normalization == 'vahadane':\n",
    "        if not os.path.exists( os.path.join(result_path, normalization + '_normalized', key)):\n",
    "            #Sample target img for normalization\n",
    "            target = cv2.imread(target_path)\n",
    "            to_transform = gauss_img\n",
    "\n",
    "            #Stain normalization\n",
    "            normalizer = staintools.StainNormalizer(method='vahadane') #method 'macenko' as alternative\n",
    "            normalizer.fit(target)\n",
    "            transformed = normalizer.transform(to_transform)\n",
    "            \n",
    "            #Save img to dict and to result folder\n",
    "            norm_dict[key] = transformed\n",
    "            path_to_result_img = os.path.join(result_path, normalization + '_normalized', key)\n",
    "            plt.imsave(path_to_result_img, transformed)\n",
    "        else:\n",
    "            print(\"already normalized\")\n",
    "            \n",
    "    elif normalization == 'macenko':\n",
    "        if not os.path.exists(os.path.join(result_path, normalization + '_normalized', key)):        \n",
    "            #Sample target img for normalization\n",
    "            target = cv2.imread(target_path)\n",
    "            to_transform = gauss_img\n",
    "        \n",
    "            #Stain normalization\n",
    "            normalizer = staintools.StainNormalizer(method='macenko') #method 'vahadane' as alternative\n",
    "            normalizer.fit(target)\n",
    "            transformed = normalizer.transform(to_transform)\n",
    "            \n",
    "            #Save img to dict and to result folder\n",
    "            norm_dict[key] = transformed\n",
    "            path_to_result_img = os.path.join(result_path, normalization + '_normalized', key)\n",
    "            plt.imsave(path_to_result_img, transformed)\n",
    "        else:\n",
    "            print(\"already normalized\")\n",
    "\n",
    "    elif normalization == 'ACE':\n",
    "        if not os.path.exists( os.path.join(result_path, normalization + '_normalized', key)):\n",
    "            to_transform = Image.open(os.path.join(dir_smooth, key))\n",
    "            #ACE normalization and Save img to result folder\n",
    "            path_to_result_img = os.path.join(result_path, normalization + '_normalized', key)\n",
    "\n",
    "            to_pil(cca.automatic_color_equalization(from_pil(to_transform))).save(path_to_result_img)\n",
    "        else:\n",
    "            print(\"already normalized\")\n",
    "            \n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqqLPr7hiuCg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############# Resizing while extracting train and test images ############\n",
    "from random import random\n",
    "\n",
    "struct_load = load_obj(dir_save = result_path_objs, file_name = 'all_norm_dicts')\n",
    "\n",
    "if struct_load == None:\n",
    "    if normalization == '':\n",
    "        norm_path = dir_smooth\n",
    "    else:\n",
    "        norm_path = os.path.join(result_path, normalization+'_normalized')\n",
    "\n",
    "    norm_dict = {}\n",
    "    norm_dict_train = {}\n",
    "    idx_train = []\n",
    "    norm_dict_test = {}\n",
    "    num_train = 50\n",
    "    count = 0\n",
    "    idx = 0\n",
    "    for filename in img_dict.keys():\n",
    "        path_to_image = os.path.join(norm_path, filename)\n",
    "        print(path_to_image)\n",
    "        \n",
    "        image = cv2.imread(path_to_image)\n",
    "        \n",
    "        scale_percent = resize # percent of original size\n",
    "        width = int(image.shape[1] * scale_percent)\n",
    "        height = int(image.shape[0] * scale_percent)\n",
    "        dim = (width, height)\n",
    "        # resize image\n",
    "        resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        norm_dict[filename] = resized\n",
    "        if (random()>0.5) & (count < num_train):\n",
    "            norm_dict_train[filename] = resized\n",
    "            idx_train.append(idx)\n",
    "            count = count+1\n",
    "        else:\n",
    "            norm_dict_test[filename] = resized\n",
    "        \n",
    "        #Save img to dict and to result folder\n",
    "        path_to_result_img = os.path.join(result_path, 'resized', filename)\n",
    "        plt.imsave(path_to_result_img, resized)\n",
    "        idx = idx+1\n",
    "else:\n",
    "    norm_dict = struct_load[0]\n",
    "    norm_dict_test = struct_load[1] \n",
    "    norm_dict_train = struct_load[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBA5pTgMDU5s",
    "outputId": "0ad8629a-6594-4344-9951-72d6200c66ae"
   },
   "outputs": [],
   "source": [
    "print(norm_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_dict_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_dict_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################### GRID SEARCH #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PX01w446iuCj"
   },
   "outputs": [],
   "source": [
    "############### OTSU MAX RADIUS PARAM SEARCH ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMAu5-RbiuCi",
    "outputId": "96d8bb93-44c2-4731-93f9-f11616ecab47"
   },
   "outputs": [],
   "source": [
    "thresholding ='OTSU'\n",
    "minR= 1\n",
    "maxR= 25\n",
    "blocksize = 0\n",
    "constant = 0\n",
    "norm_dict = norm_dict_train\n",
    "\n",
    "correl = thresholding_Hough_correl(norm_dict_train,thresholding = thresholding,min_Radius = minR,max_Radius = maxR,blocksize= None,constant = None)\n",
    "print(correl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJ8D00SniuCk"
   },
   "outputs": [],
   "source": [
    "file_name = 'maxR_OTSU_correl_dict'\n",
    "\n",
    "struct_load =  load_obj(dir_save = result_path_objs, file_name = file_name)\n",
    "\n",
    "if struct_load == None:\n",
    "    OTSU_correl_dict = {}\n",
    "    minv = 2\n",
    "    maxv = 101\n",
    "    stepv = 1\n",
    "    for maxR in range(minv,maxv,stepv):\n",
    "        correl = thresholding_Hough_correl(norm_dict_train,thresholding = 'OTSU',max_Radius=maxR)\n",
    "        key = 'maxRadius' + str(maxR)\n",
    "        OTSU_correl_dict[maxR] = correl\n",
    "\n",
    "    plt.bar(list(range(minv,maxv,stepv)), OTSU_correl_dict.values(), color='b')\n",
    "    #plt.title('OTSU MAX RADIUS CORRELATION')\n",
    "    plt.xlabel('Maximum Radius')\n",
    "    plt.ylabel('Correlation')\n",
    "    plt.savefig(os.path.join(result_path, 'OTSU_maxRadius_plot.pdf'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"all correlation Values \")\n",
    "    for k, v in OTSU_correl_dict.items(): print(k, v)\n",
    "\n",
    "    # get key with max value\n",
    "    best_maxRadius = max(OTSU_correl_dict, key=OTSU_correl_dict.get)\n",
    "\n",
    "\n",
    "    print(\"best correlation = \"+str(OTSU_correl_dict[best_maxRadius])+\" at Radius =\" + str(best_maxRadius))\n",
    "\n",
    "    save_obj(obj = (OTSU_correl_dict, best_maxRadius), dir_save = result_path_objs, file_name = 'maxR_OTSU_correl_dict')\n",
    "\n",
    "else:\n",
    "    OTSU_correl_dict = struct_load[0] \n",
    "    best_maxRadius = struct_load[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec9QF8G4iuCl"
   },
   "outputs": [],
   "source": [
    "file_name = 'minR_OTSU_correl_dict'\n",
    "\n",
    "struct_load =  load_obj(dir_save = result_path_objs, file_name = file_name)\n",
    "\n",
    "\n",
    "if struct_load == None:\n",
    "\n",
    "    min_OTSU_correl_dict = {}\n",
    "\n",
    "    minv =1\n",
    "    for minR in range(minv,best_maxRadius+1,stepv):\n",
    "        correl = thresholding_Hough_correl(norm_dict_train,thresholding = thresholding,min_Radius = minR,max_Radius=best_maxRadius)\n",
    "        min_OTSU_correl_dict[minR] = correl\n",
    "\n",
    "    plt.bar(list(range(minv,best_maxRadius+1,stepv)), min_OTSU_correl_dict.values(), color='b')\n",
    "    #plt.title('OTSU MAX RADIUS CORRELATION')\n",
    "    plt.xlabel('Minimum Radius')\n",
    "    plt.ylabel('Correlation')\n",
    "    plt.savefig(os.path.join(result_path, 'OTSU_minRadius_plot.pdf'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"all correlation Values \")\n",
    "    for k, v in min_OTSU_correl_dict.items(): print(k, v)\n",
    "\n",
    "\n",
    "\n",
    "    # get key with max value\n",
    "    best_minRadius = max(min_OTSU_correl_dict, key=min_OTSU_correl_dict.get)\n",
    "\n",
    "\n",
    "    print(\"best correlation = \"+str(min_OTSU_correl_dict[best_minRadius])+\" at Radius =\" + str(best_minRadius))\n",
    "\n",
    "\n",
    "    save_obj(obj = (min_OTSU_correl_dict, best_minRadius), dir_save = result_path_objs, file_name = 'minR_OTSU_correl_dict')\n",
    "\n",
    "else:\n",
    "    min_OTSU_correl_dict = struct_load[0] \n",
    "    best_minRadius = struct_load[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMSP10Swtq7r",
    "outputId": "a5194053-dcb5-4c5f-a616-ba3c864f4936"
   },
   "outputs": [],
   "source": [
    "print(best_minRadius, best_maxRadius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "posvkHMRiuCm"
   },
   "outputs": [],
   "source": [
    "##################### TWO PARAM OTSU GRID PARAM SEARCH ######################À"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pncqx4FjiuCm",
    "outputId": "32ca2961-27b3-410f-c958-6584ca5931fa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "struct_load = load_obj(dir_save = result_path_objs, file_name = 'twoWay_OTSU_correl_dict')\n",
    "maxv = 75\n",
    "if struct_load ==None:\n",
    "\n",
    "    #if struct_load ==None:\n",
    "    two_param_OTSU_correl_dict = {}\n",
    "    maxv = 75\n",
    "    df_otsu = np.zeros(shape=(maxv,maxv))\n",
    "\n",
    "    idx_minR =-1\n",
    "    for min_Radius in range(1,maxv+1,1):\n",
    "        idx_minR = idx_minR + 1\n",
    "        idx_maxR =-1\n",
    "        for max_Radius in range(1,maxv+1,1):\n",
    "            idx_maxR = idx_maxR + 1\n",
    "            correl = thresholding_Hough_correl(norm_dict_train,'OTSU',min_Radius,max_Radius)\n",
    "            key = (min_Radius,max_Radius)\n",
    "            two_param_OTSU_correl_dict[key] = correl\n",
    "            df_otsu[idx_minR, idx_maxR] = correl\n",
    "        \n",
    "    # get key with max value\n",
    "    (best_minR_twoWay, best_maxR_twoWay) = max(two_param_OTSU_correl_dict, key=two_param_OTSU_correl_dict.get)\n",
    "\n",
    "    print(\"best correlation = \"+str(two_param_OTSU_correl_dict[(best_minR_twoWay, best_maxR_twoWay)])+\" at Radiuses =\" + str((best_minR_twoWay, best_maxR_twoWay)))\n",
    "    save_obj( obj = (two_param_OTSU_correl_dict, df_otsu, best_minR_twoWay, best_maxR_twoWay), dir_save = result_path_objs, file_name = 'twoWay_OTSU_correl_dict')\n",
    "else:\n",
    "    two_param_OTSU_correl_dict = struct_load[0] \n",
    "    df_otsu = struct_load[1]\n",
    "#    (best_minR_twoWay, best_maxR_twoWay) = max(df_otsu, key=df_otsu.get)\n",
    "    best_minR_twoWay = struct_load[2] \n",
    "    best_maxR_twoWay = struct_load[3]\n",
    "\n",
    "print('\\n'+'correlation on train images with best params from 2d grid search:' +str(df_otsu[best_minR_twoWay-1,best_maxR_twoWay-1]))\n",
    "#################### OTSU CORRELATION ON TEST IMAGES WITH BEST PARAMS #########################################\n",
    "\n",
    "correl = thresholding_Hough_correl(norm_dict_test,thresholding = thresholding,min_Radius = best_minR_twoWay, max_Radius=best_maxR_twoWay)\n",
    "\n",
    "print(\"correlation on test images with best params from 2D grid search: minR = \"+str(best_minR_twoWay)+\" - maxR = \"+ str(best_maxR_twoWay) + \" = \" + str(correl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_index = np.where(df_otsu == two_param_OTSU_correl_dict[(best_minR_twoWay, best_maxR_twoWay)])\n",
    "best_MinR_idx = best_params_index[0][0]\n",
    "best_MaxR_idx = best_params_index[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zr8vLLr9TFyo",
    "outputId": "1bc14604-34e4-46ea-85d9-3988a9c6925a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax_bar = plt.subplots(4,1, figsize=(8,32))\n",
    "\n",
    "ax_bar[0].bar(np.arange(1,maxv+1,1),df_otsu[:,best_MaxR_idx])\n",
    "ax_bar[0].set_title('OTSU - correlation values with fixed max radius = '+str(best_maxR_twoWay))\n",
    "\n",
    "ax_bar[1].plot(np.arange(1,maxv+1,1),df_otsu[:, best_maxR_twoWay-1], label='best maxR = ' + str(best_maxR_twoWay))\n",
    "for j in np.arange(1,10,1):\n",
    "    ax_bar[1].plot(np.arange(1,maxv+1,1),df_otsu[:, best_MaxR_idx-j], '-.', alpha = 0.4,  label=\"maxR = \" +str(best_maxR_twoWay-j))\n",
    "    ax_bar[1].plot(np.arange(1,maxv+1,1),df_otsu[:, best_MaxR_idx+j], '-.', alpha = 0.4, label=\"maxR = \" +str(best_maxR_twoWay+j))\n",
    "ax_bar[1].legend(loc = 'upper right')\n",
    "ax_bar[1].set_title('OTSU - correlation for different fixed values of max radius')\n",
    "\n",
    "\n",
    "ax_bar[2].bar(np.arange(1,maxv+1,1),df_otsu[best_MinR_idx,:])\n",
    "ax_bar[2].set_title('OTSU - correlation values with fixed min radius = '+str(best_minR_twoWay))\n",
    "\n",
    "ax_bar[3].plot(np.arange(1,maxv+1,1),df_otsu[best_MinR_idx,:], label='best minR = ' + str(best_minR_twoWay))\n",
    "for j in np.arange(1,10,1):\n",
    "    if (best_minR_twoWay-j >0):\n",
    "        ax_bar[3].plot(np.arange(1,maxv+1,1),df_otsu[best_MinR_idx-j,:], '-.', alpha = 0.4,  label=\"minR = \" +str(best_minR_twoWay-j))\n",
    "    ax_bar[3].plot(np.arange(1,maxv+1,1),df_otsu[best_MinR_idx+j,:], '-.', alpha = 0.4, label=\"minR = \" +str(best_minR_twoWay+j))\n",
    "ax_bar[3].legend(loc = 'lower right')\n",
    "ax_bar[3].set_title('OTSU - correlation for different fixed values of min radius')\n",
    "\n",
    "plt.savefig(os.path.join(result_path, 'OTSU_minMaxRadius_distribution.pdf'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "tGff2rYwiuCs",
    "outputId": "96625a3c-ee9a-4aa0-899a-7622da5e77b9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(8,8))\n",
    "\n",
    "# Make data.\n",
    "maxMinR = 14\n",
    "maxMaxR = 40\n",
    "Y = np.arange(1,maxMinR, 1)\n",
    "X = np.arange(1,maxMaxR, 1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "df_otsu[np.where(np.isnan(df_otsu))] = 0\n",
    "Z = df_otsu[np.arange(0,maxMinR-1,1),:]\n",
    "Z = Z[:, np.arange(0,maxMaxR-1,1)]\n",
    "\n",
    "print(X.shape, Y.shape, Z.shape)\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "\n",
    "# Customize the z axis.\n",
    "ax.set_zlim(-1.00, 1.00)\n",
    "#ax.zaxis.set_major_locator(LinearLocator(0))\n",
    "# A StrMethodFormatter is used automatically\n",
    "#ax.zaxis.set_major_formatter('{x:.02f}')\n",
    "ax.set_xlabel('max Radius')\n",
    "ax.set_ylabel('min Radius')\n",
    "#ax.view_init(20, +30)\n",
    "ax.view_init(30, +45)\n",
    "#ax.set_title('OTSU MIN MAX RADIUS CORRELATION')\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.savefig(os.path.join(result_path, 'OTSU_minMaxRadius_surface.pdf'))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9dlUapuq_4O",
    "outputId": "dae86c09-5988-464d-b9ec-5d2200906f0f"
   },
   "outputs": [],
   "source": [
    "print(df_otsu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTYNayv_iuCt"
   },
   "outputs": [],
   "source": [
    "################################# ADAPTIVE THRESHOLDING ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0rf3CUviuCu"
   },
   "outputs": [],
   "source": [
    "######################## ADAPTIVE MEAN THREE PARAMS GRID SEARCH######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIZ78UfMiuCu",
    "outputId": "1307dc4d-4895-4237-832d-2ce86016f64b"
   },
   "outputs": [],
   "source": [
    "maxC = 60\n",
    "minBsize = 3\n",
    "maxBsize = 50\n",
    "\n",
    "maxRadius = 50\n",
    "minR =  best_minR_twoWay\n",
    "struct_load = load_obj(dir_save = result_path_objs, file_name = 'adaptive_mean_correl_dict')\n",
    "\n",
    "if struct_load==None:\n",
    "\n",
    "    adaptive_mean_correl_dict = {}\n",
    "\n",
    "    df_adaptive_mean = np.zeros(shape=(len(range(0,maxC,4)),len(range(minBsize,maxBsize,2)), 1, maxRadius ))\n",
    "    \n",
    "    idx_C =-1   \n",
    "    #for maxRadius in range(11,41,2):\n",
    "    for C in range(0,maxC,4):\n",
    "        idx_C = idx_C + 1  \n",
    "        idx_b =-1\n",
    "        for bsize in range(minBsize,maxBsize,4):\n",
    "            idx_b = idx_b + 1\n",
    "            #for minR in best_minR_twoWay:\n",
    "            for maxR in range(15,45,1):            \n",
    "                correl = thresholding_Hough_correl(norm_dict_train, thresholding = 'ADAPTIVE_MEAN', min_Radius = minR , max_Radius = maxR, blocksize= bsize, constant=  C)\n",
    "                adaptive_mean_correl_dict[(bsize, C, minR, maxR)] = correl  \n",
    "                df_adaptive_mean[idx_C, idx_b, 0, maxR-1] = correl\n",
    "\n",
    "    save_obj( obj = (adaptive_mean_correl_dict, df_adaptive_mean), dir_save = result_path_objs, file_name = 'adaptive_mean_correl_dict')\n",
    "else:\n",
    "    adaptive_mean_correl_dict = struct_load[0] \n",
    "    df_adaptive_mean = struct_load[1]\n",
    "\n",
    "    # get key with max value\n",
    "(best_b_size_adaptive, best_C_adaptive, best_minR_adaptive, best_maxR_adaptive) = max(adaptive_mean_correl_dict, key=adaptive_mean_correl_dict.get)\n",
    "\n",
    "print(\"best correlation on training set = \"+str(adaptive_mean_correl_dict[(best_b_size_adaptive, best_C_adaptive, best_minR_adaptive, best_maxR_adaptive)])+\" at Radiuses =\" , str(best_minR_adaptive)+ \", \"+str(best_maxR_adaptive)+ \" - C = \"+str(best_C_adaptive)+ \" - blocksize = \"+str(best_b_size_adaptive))\n",
    "\n",
    "\n",
    "\n",
    "#################### ADAPTIVE MEAN ON TEST IMAGES WITH BEST PARAMS #########################################\n",
    "\n",
    "correl = thresholding_Hough_correl(norm_dict_test,thresholding = 'ADAPTIVE_MEAN', min_Radius = best_minR_adaptive, max_Radius= best_maxR_adaptive, blocksize = best_b_size_adaptive, constant = best_C_adaptive)\n",
    "\n",
    "print(\"correlation on test images [with best params: minR = \"+str(best_minR_adaptive)+\" - maxR = \"+ str(best_maxR_adaptive) + \" - C = \"+ str(best_C_adaptive) + \" - blocksize\"+ str(best_b_size_adaptive) + \"] = \" + str(correl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mCToxkKi30T"
   },
   "outputs": [],
   "source": [
    "######################## ADAPTIVE MEAN BEST C AND BSIZE RADIUSES SEARCH######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d93EebukLD-n",
    "outputId": "06cdd0b0-5e5e-4d87-d643-75a5f4a588a9"
   },
   "outputs": [],
   "source": [
    "bsize = best_b_size_adaptive\n",
    "C = best_C_adaptive    \n",
    "maxRadius = 50\n",
    "\n",
    "struct_load = load_obj(dir_save = result_path_objs, file_name = 'adaptive_mean_correl_dict_radiuses')\n",
    "\n",
    "if struct_load==None:\n",
    "    print('create dict')\n",
    "    adaptive_mean_correl_dict = {}\n",
    "    df_adaptive_mean = np.zeros(shape=(maxRadius, maxRadius ))\n",
    "    for minR in range(1,maxRadius,1):\n",
    "        for maxR in range(minR+1,maxRadius,1):            \n",
    "            correl = thresholding_Hough_correl(norm_dict_train, thresholding = 'ADAPTIVE_MEAN', min_Radius = minR , max_Radius = maxR, blocksize= bsize, constant=  C)\n",
    "            adaptive_mean_correl_dict[(minR, maxR)] = correl  \n",
    "            df_adaptive_mean[minR-1, maxR-1] = correl\n",
    "\n",
    "    save_obj( obj = (adaptive_mean_correl_dict, df_adaptive_mean), dir_save = result_path_objs, file_name = 'adaptive_mean_correl_dict_radiuses')\n",
    "else:\n",
    "    adaptive_mean_correl_dict = struct_load[0] \n",
    "    df_adaptive_mean = struct_load[1]\n",
    "\n",
    "    # get key with max value\n",
    "(best_minR_adaptive, best_maxR_adaptive) = max(adaptive_mean_correl_dict, key=adaptive_mean_correl_dict.get)\n",
    "\n",
    "print(\"best correlation = \"+str(adaptive_mean_correl_dict[(best_minR_adaptive, best_maxR_adaptive)])+\" at Radiuses =\" , str(best_minR_adaptive)+ \", \"+str(best_maxR_adaptive)+ \" - C = \"+str(best_C_adaptive)+ \" - blocksize = \"+str(best_b_size_adaptive))\n",
    "\n",
    "\n",
    "\n",
    "#################### ADAPTIVE MEAN CORRELATION ON TEST IMAGES WITH BEST PARAMS #########################################\n",
    "\n",
    "correl = thresholding_Hough_correl(norm_dict_test,thresholding = 'ADAPTIVE_MEAN', min_Radius = best_minR_adaptive, max_Radius= best_maxR_adaptive, blocksize = best_b_size_adaptive, constant = best_C_adaptive)\n",
    "\n",
    "print(\"correlation on test images [with best params: minR = \"+str(best_minR_adaptive)+\" - maxR = \"+ str(best_maxR_adaptive) + \" - C = \"+ str(best_C_adaptive) + \" - blocksize\"+ str(best_b_size_adaptive) + \"] = \" + str(correl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUky1Yp0GjrO"
   },
   "outputs": [],
   "source": [
    "############ GLOBAL ADAPTIVE MEAN PARAM SEARCH ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xTE6WfqUnNmg",
    "outputId": "1eb916b8-65a0-453a-aeea-cfb56d64e0f7"
   },
   "outputs": [],
   "source": [
    "struct_load = load_obj(dir_save = result_path_objs, file_name = 'global_adaptive_mean_correl_dict')\n",
    "\n",
    "if struct_load==None:\n",
    "\n",
    "    adaptive_mean_correl_dict = {}\n",
    "\n",
    "    df_adaptive_mean = np.zeros(shape=(10, 5, 9, 9))\n",
    "\n",
    "    idx_C =-1   \n",
    "    #for maxRadius in range(11,41,2):\n",
    "    for C in range(best_C_adaptive-9,best_C_adaptive+10,2):\n",
    "        idx_C = idx_C + 1  \n",
    "        idx_b =-1\n",
    "        if best_b_size_adaptive == 3:\n",
    "            for bsize in range(best_b_size_adaptive,best_b_size_adaptive+9,2):\n",
    "                idx_b = idx_b + 1\n",
    "                idx_minR =-1\n",
    "                for minR in range(best_minR_adaptive-4,best_minR_adaptive+5,1):\n",
    "                    idx_minR = idx_minR + 1\n",
    "                    idx_maxR =-1\n",
    "                    for maxR in range(best_maxR_adaptive-4,best_maxR_adaptive+5,1):\n",
    "                        idx_maxR = idx_maxR + 1\n",
    "                        correl = thresholding_Hough_correl(norm_dict_train, thresholding = 'ADAPTIVE_GAUSSIAN', min_Radius = minR , max_Radius = maxR, blocksize= bsize, constant=  C)\n",
    "                        adaptive_mean_correl_dict[(bsize, C, minR, maxR)] = correl  \n",
    "                        df_adaptive_mean[idx_C, idx_b, idx_minR, idx_maxR] = correl\n",
    "        else:\n",
    "            for bsize in range(best_b_size_adaptive-4,best_b_size_adaptive+5,2):\n",
    "                idx_b = idx_b + 1\n",
    "                idx_minR =-1\n",
    "                for minR in range(best_minR_adaptive-4,best_minR_adaptive+5,1):\n",
    "                    idx_minR = idx_minR + 1\n",
    "                    idx_maxR =-1\n",
    "                    for maxR in range(best_maxR_adaptive-4,best_maxR_adaptive+5,1):\n",
    "                        idx_maxR = idx_maxR + 1\n",
    "                        correl = thresholding_Hough_correl(norm_dict_train, thresholding = 'ADAPTIVE_GAUSSIAN', min_Radius = minR , max_Radius = maxR, blocksize= bsize, constant=  C)\n",
    "                        adaptive_mean_correl_dict[(bsize, C, minR, maxR)] = correl  \n",
    "                        df_adaptive_mean[idx_C, idx_b, idx_minR, idx_maxR] = correl\n",
    "\n",
    "    save_obj( obj = (adaptive_mean_correl_dict, df_adaptive_mean), dir_save = result_path_objs, file_name = 'global_adaptive_mean_correl_dict')\n",
    "else:\n",
    "    adaptive_mean_correl_dict = struct_load[0] \n",
    "    df_adaptive_mean = struct_load[1]\n",
    "\n",
    "    # get key with max value\n",
    "(best_b_size_adaptive, best_C_adaptive, best_minR_adaptive, best_maxR_adaptive) = max(adaptive_mean_correl_dict, key=adaptive_mean_correl_dict.get)\n",
    "\n",
    "print(\"best correlation on training set = \"+str(adaptive_mean_correl_dict[(best_b_size_adaptive, best_C_adaptive, best_minR_adaptive, best_maxR_adaptive)])+\" at Radiuses =\" , str(best_minR_adaptive)+ \", \"+str(best_maxR_adaptive)+ \" - C = \"+str(best_C_adaptive)+ \" - blocksize = \"+str(best_b_size_adaptive))\n",
    "\n",
    "\n",
    "\n",
    "#################### ADAPTIVE MEAN ON TEST IMAGES WITH BEST PARAMS #########################################\n",
    "\n",
    "correl = thresholding_Hough_correl(norm_dict_test,thresholding = 'ADAPTIVE_MEAN', min_Radius = best_minR_adaptive, max_Radius= best_maxR_adaptive, blocksize = best_b_size_adaptive, constant = best_C_adaptive)\n",
    "\n",
    "print(\"correlation on test images [with best params: minR = \"+str(best_minR_adaptive)+\" - maxR = \"+ str(best_maxR_adaptive) + \" - C = \"+ str(best_C_adaptive) + \" - blocksize\"+ str(best_b_size_adaptive) + \"] = \" + str(correl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bq5bPAsIiYOG",
    "outputId": "ec86127b-399b-471c-fb95-4bbcbcd6c4d4"
   },
   "outputs": [],
   "source": [
    "best_params_index = np.where(df_adaptive_mean == adaptive_mean_correl_dict[(best_b_size_adaptive, best_C_adaptive, best_minR_adaptive, best_maxR_adaptive)])\n",
    "best_B_idx = best_params_index[0][0]\n",
    "best_C_idx = best_params_index[1][0]\n",
    "best_MinR_idx = best_params_index[2][0]\n",
    "best_MaxR_idx = best_params_index[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CSNhuwEBeuAc",
    "outputId": "1f588eac-185e-48e1-8d68-11ec2cab26e5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax_bar = plt.subplots(4,1, figsize=(8,32))\n",
    "\n",
    "ax_bar[0].bar(np.arange(best_minR_adaptive-4,best_minR_adaptive+5,1),df_adaptive_mean[best_B_idx,best_C_idx, :, best_MaxR_idx])\n",
    "ax_bar[0].set_title('AMT - correlation values with fixed max radius = '+str(best_maxR_adaptive))\n",
    "\n",
    "ax_bar[1].plot(np.arange(best_minR_adaptive-4,best_minR_adaptive+5,1),df_adaptive_mean[best_B_idx,best_C_idx, :, best_MaxR_idx], label='best maxR = ' + str(best_maxR_adaptive))\n",
    "for j in np.arange(1,4,1):\n",
    "    ax_bar[1].plot(np.arange(best_minR_adaptive-4,best_minR_adaptive+5,1),df_adaptive_mean[best_B_idx,best_C_idx, :, best_MaxR_idx], '-.', alpha = 0.4,  label=\"maxR = \" +str(best_maxR_adaptive-j))\n",
    "    ax_bar[1].plot(np.arange(best_minR_adaptive-4,best_minR_adaptive+5,1),df_adaptive_mean[best_B_idx,best_C_idx, :, best_MaxR_idx], '-.', alpha = 0.4, label=\"maxR = \" +str(best_maxR_adaptive+j))\n",
    "ax_bar[1].legend(loc = 'upper right')\n",
    "ax_bar[1].set_title('AMT - correlation for different fixed values of max radius')\n",
    "\n",
    "\n",
    "ax_bar[2].bar(np.arange(best_maxR_adaptive-4,best_maxR_adaptive+5,1),df_adaptive_mean[best_B_idx,best_C_idx, best_MinR_idx, :])\n",
    "ax_bar[2].set_title('AMT - correlation values with fixed min radius = '+str(best_minR_adaptive))\n",
    "\n",
    "ax_bar[3].plot(np.arange(best_maxR_adaptive-4,best_maxR_adaptive+5,1),df_adaptive_mean[best_B_idx,best_C_idx, best_MinR_idx, :], label='best minR = ' + str(best_minR_adaptive))\n",
    "for j in np.arange(1,4,1):\n",
    "    if (best_minR_adaptive-j >0):\n",
    "        ax_bar[3].plot(np.arange(best_maxR_adaptive-4,best_maxR_adaptive+5,1),df_adaptive_mean[best_B_idx,best_C_idx, best_MinR_idx-1, :], '-.', alpha = 0.4,  label=\"minR = \" +str(best_minR_adaptive-j))\n",
    "        ax_bar[3].plot(np.arange(best_maxR_adaptive-4,best_maxR_adaptive+5,1),df_adaptive_mean[best_B_idx,best_C_idx, best_MinR_idx-1, :], '-.', alpha = 0.4, label=\"minR = \" +str(best_minR_adaptive+j))\n",
    "ax_bar[3].legend(loc = 'lower right')\n",
    "ax_bar[3].set_title('AMT - correlation for different fixed values of min radius')\n",
    "\n",
    "plt.savefig(os.path.join(result_path, 'AMT_minMaxRadius_distribution.pdf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSCISocAiwqn"
   },
   "outputs": [],
   "source": [
    "######################## ADAPTIVE GAUSSIAN THREE PARAMS GRID SEARCH######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-NwrSXybLP0Y",
    "outputId": "6cf0a3d8-1dc1-4f7b-8800-e71de5628ebc"
   },
   "outputs": [],
   "source": [
    "maxC = 60\n",
    "minBsize = 3\n",
    "maxBsize = 50\n",
    "\n",
    "maxRadius = 50\n",
    "minR =  best_minR_twoWay\n",
    "struct_load = load_obj(dir_save = result_path_objs, file_name = 'adaptive_gauss_correl_dict')\n",
    "\n",
    "if struct_load==None:\n",
    "\n",
    "    adaptive_gauss_correl_dict = {}\n",
    "\n",
    "    df_adaptive_gauss = np.zeros(shape=(len(range(0,maxC,4)),len(range(minBsize,maxBsize,2)), 1, maxRadius ))\n",
    "    \n",
    "    idx_C =-1   \n",
    "    #for maxRadius in range(11,41,2):\n",
    "    for C in range(0,maxC,4):\n",
    "        idx_C = idx_C + 1  \n",
    "        idx_b =-1\n",
    "        for bsize in range(minBsize,maxBsize,4):\n",
    "            idx_b = idx_b + 1\n",
    "            #for minR in best_minR_twoWay:\n",
    "            for maxR in range(15,45,1):            \n",
    "                correl = thresholding_Hough_correl(norm_dict_train, thresholding = 'ADAPTIVE_GAUSSIAN', min_Radius = minR , max_Radius = maxR, blocksize= bsize, constant=  C)\n",
    "                adaptive_gauss_correl_dict[(bsize, C, minR, maxR)] = correl  \n",
    "                df_adaptive_gauss[idx_C, idx_b, 0, maxR-1] = correl\n",
    "\n",
    "    save_obj( obj = (adaptive_gauss_correl_dict, df_adaptive_gauss), dir_save = result_path_objs, file_name = 'adaptive_gauss_correl_dict')\n",
    "else:\n",
    "    adaptive_gauss_correl_dict = struct_load[0] \n",
    "    df_adaptive_gauss = struct_load[1]\n",
    "\n",
    "    # get key with max value\n",
    "(best_b_size_adaptive, best_C_adaptive, best_minR_adaptive, best_maxR_adaptive) = max(adaptive_gauss_correl_dict, key=adaptive_gauss_correl_dict.get)\n",
    "\n",
    "print(\"best correlation on training set = \"+str(adaptive_gauss_correl_dict[(best_b_size_adaptive, best_C_adaptive, best_minR_adaptive, best_maxR_adaptive)])+\" at Radiuses =\" , str(best_minR_adaptive)+ \", \"+str(best_maxR_adaptive)+ \" - C = \"+str(best_C_adaptive)+ \" - blocksize = \"+str(best_b_size_adaptive))\n",
    "\n",
    "\n",
    "\n",
    "#################### ADAPTIVE_GAUSSIAN ON TEST IMAGES WITH BEST PARAMS #########################################\n",
    "\n",
    "correl = thresholding_Hough_correl(norm_dict_test,thresholding = 'ADAPTIVE_GAUSSIAN', min_Radius = best_minR_adaptive, max_Radius= best_maxR_adaptive, blocksize = best_b_size_adaptive, constant = best_C_adaptive)\n",
    "\n",
    "print(\"correlation on test images [with best params: minR = \"+str(best_minR_adaptive)+\" - maxR = \"+ str(best_maxR_adaptive) + \" - C = \"+ str(best_C_adaptive) + \" - blocksize\"+ str(best_b_size_adaptive) + \"] = \" + str(correl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mc26r3xM9j2A"
   },
   "outputs": [],
   "source": [
    "######################### ADAPTIVE GAUSSIAN BEST C AND BSIZE RADIUSES SEARCH######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7dmzlZX9Gpx",
    "outputId": "01568059-c9e4-491f-a219-09509420bc44"
   },
   "outputs": [],
   "source": [
    "bsize = best_b_size_adaptive\n",
    "C = best_C_adaptive    \n",
    "maxRadius = 50\n",
    "\n",
    "struct_load = load_obj(dir_save = result_path_objs, file_name = 'adaptive_gauss_correl_dict_radiuses')\n",
    "\n",
    "if struct_load==None:\n",
    "    print('create dict')\n",
    "    adaptive_mean_correl_dict = {}\n",
    "    df_adaptive_mean = np.zeros(shape=(maxRadius, maxRadius ))\n",
    "    for minR in range(1,maxRadius,1):\n",
    "        for maxR in range(minR+1,maxRadius,1):            \n",
    "            correl = thresholding_Hough_correl(norm_dict_train, thresholding = 'ADAPTIVE_GAUSSIAN', min_Radius = minR , max_Radius = maxR, blocksize= bsize, constant=  C)\n",
    "            adaptive_mean_correl_dict[(minR, maxR)] = correl  \n",
    "            df_adaptive_mean[minR-1, maxR-1] = correl\n",
    "\n",
    "    save_obj( obj = (adaptive_mean_correl_dict, df_adaptive_mean), dir_save = result_path_objs, file_name = 'adaptive_gauss_correl_dict_radiuses')\n",
    "else:\n",
    "    adaptive_mean_correl_dict = struct_load[0] \n",
    "    df_adaptive_mean = struct_load[1]\n",
    "\n",
    "    # get key with max value\n",
    "(best_minR_adaptive, best_maxR_adaptive) = max(adaptive_mean_correl_dict, key=adaptive_mean_correl_dict.get)\n",
    "\n",
    "print(\"best correlation = \"+str(adaptive_mean_correl_dict[(best_minR_adaptive, best_maxR_adaptive)])+\" at Radiuses =\" , str(best_minR_adaptive)+ \", \"+str(best_maxR_adaptive)+ \" - C = \"+str(best_C_adaptive)+ \" - blocksize = \"+str(best_b_size_adaptive))\n",
    "\n",
    "\n",
    "\n",
    "#################### ADAPTIVE GAUSSIAN CORRELATION ON TEST IMAGES WITH BEST PARAMS #########################################\n",
    "\n",
    "correl = thresholding_Hough_correl(norm_dict_test,thresholding = 'ADAPTIVE_GAUSSIAN', min_Radius = best_minR_adaptive, max_Radius= best_maxR_adaptive, blocksize = best_b_size_adaptive, constant = best_C_adaptive)\n",
    "\n",
    "print(\"correlation on test images [with best params: minR = \"+str(best_minR_adaptive)+\" - maxR = \"+ str(best_maxR_adaptive) + \" - C = \"+ str(best_C_adaptive) + \" - blocksize\"+ str(best_b_size_adaptive) + \"] = \" + str(correl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0pm03FzwFbO"
   },
   "outputs": [],
   "source": [
    "############ GLOBAL ADAPTIVE GAUSSIAN PARAM SEARCH ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9zBvsj9D7Kr",
    "outputId": "a06e1718-2fda-4a96-b515-bfc1425b27ba"
   },
   "outputs": [],
   "source": [
    "struct_load = load_obj(dir_save = result_path_objs, file_name = 'global_adaptive_gauss_correl_dict')\n",
    "\n",
    "if struct_load==None:\n",
    "\n",
    "    adaptive_gauss_correl_dict = {}\n",
    "\n",
    "    df_adaptive_gauss = np.zeros(shape=(10, 5, 9, 9))\n",
    "\n",
    "    idx_C =-1   \n",
    "    for C in range(best_C_adaptive-9,best_C_adaptive+10,2):\n",
    "        idx_C = idx_C + 1  \n",
    "        idx_b =-1\n",
    "        if best_b_size_adaptive == 3:\n",
    "            for bsize in range(best_b_size_adaptive,best_b_size_adaptive+9,2):\n",
    "                idx_b = idx_b + 1\n",
    "                idx_minR =-1\n",
    "                for minR in range(best_minR_adaptive-4,best_minR_adaptive+5,1):\n",
    "                    idx_minR = idx_minR + 1\n",
    "                    idx_maxR =-1\n",
    "                    for maxR in range(best_maxR_adaptive-4,best_maxR_adaptive+5,1):\n",
    "                        idx_maxR = idx_maxR + 1\n",
    "                        correl = thresholding_Hough_correl(norm_dict_train, thresholding = 'ADAPTIVE_GAUSSIAN', min_Radius = minR , max_Radius = maxR, blocksize= bsize, constant=  C)\n",
    "                        adaptive_gauss_correl_dict[(bsize, C, minR, maxR)] = correl  \n",
    "                        df_adaptive_gauss[idx_C, idx_b, idx_minR, idx_maxR] = correl\n",
    "        else:\n",
    "            for bsize in range(best_b_size_adaptive-4,best_b_size_adaptive+5,2):\n",
    "                idx_b = idx_b + 1\n",
    "                idx_minR =-1\n",
    "                for minR in range(best_minR_adaptive-4,best_minR_adaptive+5,1):\n",
    "                    idx_minR = idx_minR + 1\n",
    "                    idx_maxR =-1\n",
    "                    for maxR in range(best_maxR_adaptive-4,best_maxR_adaptive+5,1):\n",
    "                        idx_maxR = idx_maxR + 1\n",
    "                        correl = thresholding_Hough_correl(norm_dict_train, thresholding = 'ADAPTIVE_GAUSSIAN', min_Radius = minR , max_Radius = maxR, blocksize= bsize, constant=  C)\n",
    "                        adaptive_gauss_correl_dict[(bsize, C, minR, maxR)] = correl  \n",
    "                        df_adaptive_gauss[idx_C, idx_b, idx_minR, idx_maxR] = correl\n",
    "\n",
    "    save_obj(obj = (adaptive_gauss_correl_dict, df_adaptive_gauss), dir_save = result_path_objs, file_name = 'global_adaptive_gauss_correl_dict')\n",
    "else:\n",
    "    adaptive_gauss_correl_dict = struct_load[0] \n",
    "    df_adaptive_gauss = struct_load[1]\n",
    "\n",
    "    # get key with max value\n",
    "(best_b_size_adaptive, best_C_adaptive, best_minR_adaptive, best_maxR_adaptive) = max(adaptive_gauss_correl_dict, key=adaptive_gauss_correl_dict.get)\n",
    "\n",
    "print(\"best correlation on training set = \"+str(adaptive_gauss_correl_dict[(best_b_size_adaptive, best_C_adaptive, best_minR_adaptive, best_maxR_adaptive)])+\" at Radiuses =\" , str(best_minR_adaptive)+ \", \"+str(best_maxR_adaptive)+ \" - C = \"+str(best_C_adaptive)+ \" - blocksize = \"+str(best_b_size_adaptive))\n",
    "\n",
    "\n",
    "\n",
    "#################### ADAPTIVE GAUSSIAN ON TEST IMAGES WITH BEST PARAMS #########################################\n",
    "\n",
    "correl = thresholding_Hough_correl(norm_dict_test,thresholding = 'ADAPTIVE_GAUSSIAN', min_Radius = best_minR_adaptive, max_Radius= best_maxR_adaptive, blocksize = best_b_size_adaptive, constant = best_C_adaptive)\n",
    "\n",
    "print(\"correlation on test images [with best params: minR = \"+str(best_minR_adaptive)+\" - maxR = \"+ str(best_maxR_adaptive) + \" - C = \"+ str(best_C_adaptive) + \" - blocksize\"+ str(best_b_size_adaptive) + \"] = \" + str(correl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_index = np.where(df_adaptive_gauss == adaptive_gauss_correl_dict[(best_b_size_adaptive, best_C_adaptive, best_minR_adaptive, best_maxR_adaptive)])\n",
    "best_B_idx = best_params_index[0][0]\n",
    "best_C_idx = best_params_index[1][0]\n",
    "best_MinR_idx = best_params_index[2][0]\n",
    "best_MaxR_idx = best_params_index[3][0]\n",
    "best_params_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adaptive_gauss[best_B_idx, best_C_idx, best_MinR_idx, best_MaxR_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m6cTAxNGVHw8",
    "outputId": "076421d9-6342-4342-e901-d1027aa1a37e"
   },
   "outputs": [],
   "source": [
    "fig, ax_bar = plt.subplots(4,1, figsize=(8,32))\n",
    "\n",
    "ax_bar[0].bar(np.arange(best_minR_adaptive-4,best_minR_adaptive+5,1),df_adaptive_gauss[best_B_idx, best_C_idx, :, best_MaxR_idx])\n",
    "ax_bar[0].set_title('AGT - correlation values with fixed max radius = '+str(best_maxR_adaptive))\n",
    "\n",
    "ax_bar[1].plot(np.arange(best_minR_adaptive-4,best_minR_adaptive+5,1),df_adaptive_gauss[best_B_idx, best_C_idx, :, best_MaxR_idx], label='best maxR = ' + str(best_maxR_adaptive))\n",
    "for j in np.arange(1,4,1):\n",
    "    ax_bar[1].plot(np.arange(best_minR_adaptive-4,best_minR_adaptive+5,1),df_adaptive_gauss[best_B_idx, best_C_idx, :, best_MaxR_idx-j], '-.', alpha = 0.4,  label=\"maxR = \" +str(best_maxR_adaptive-j))\n",
    "    ax_bar[1].plot(np.arange(best_minR_adaptive-4,best_minR_adaptive+5,1),df_adaptive_gauss[best_B_idx, best_C_idx, :, best_MaxR_idx+j], '-.', alpha = 0.4, label=\"maxR = \" +str(best_maxR_adaptive+j))\n",
    "ax_bar[1].legend(loc = 'upper right')\n",
    "ax_bar[1].set_title('AGT - correlation for different fixed values of max radius')\n",
    "\n",
    "\n",
    "ax_bar[2].bar(np.arange(best_maxR_adaptive-4,best_maxR_adaptive+5,1),df_adaptive_gauss[best_B_idx, best_C_idx, best_MinR_idx, :])\n",
    "ax_bar[2].set_title('AGT - correlation values with fixed min radius = '+str(best_minR_adaptive))\n",
    "\n",
    "ax_bar[3].plot(np.arange(best_maxR_adaptive-4,best_maxR_adaptive+5,1),df_adaptive_gauss[best_B_idx, best_C_idx, best_MinR_idx, :], label='best minR = ' + str(best_minR_adaptive))\n",
    "for j in np.arange(1,4,1):\n",
    "    if (best_minR_adaptive-j >0):\n",
    "        ax_bar[3].plot(np.arange(best_maxR_adaptive-4,best_maxR_adaptive+5,1),df_adaptive_gauss[best_B_idx, best_C_idx, best_MinR_idx-j, :], '-.', alpha = 0.4,  label=\"minR = \" +str(best_minR_adaptive-j))\n",
    "        ax_bar[3].plot(np.arange(best_maxR_adaptive-4,best_maxR_adaptive+5,1),df_adaptive_gauss[best_B_idx, best_C_idx, best_MinR_idx+j, :], '-.', alpha = 0.4, label=\"minR = \" +str(best_minR_adaptive+j))\n",
    "ax_bar[3].legend(loc = 'lower right')\n",
    "ax_bar[3].set_title('AGT - correlation for different fixed values of min radius')\n",
    "\n",
    "plt.savefig(os.path.join(result_path, 'AGT_minMaxRadius_distribution.pdf'))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Comb_Image_Pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
